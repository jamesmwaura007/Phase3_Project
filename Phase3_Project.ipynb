{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5e5797",
   "metadata": {},
   "source": [
    "# Phase3 Machine Learning Project\n",
    "### James Kimani Mwaura\n",
    "## SyriaTel Customer Churn Prediction\n",
    "### Business Understanding\n",
    "\n",
    "In this project, my aim is to predict customer churn for SyriaTel, a telecommunications company.\n",
    "Churn prediction helps businesses retain customers by identifying factors leading to customer attrition.\n",
    "\n",
    "**Brief History of the SyriaTel**\n",
    "Syriatel was founded in January 2000, with its headquarters in Damascus, Syria. The Government licenses two private companies to supply mobile phone services, Syriatel and \"94\".\n",
    "\n",
    "**Company Ownwership**\n",
    "The company is owned by Rami Makhlouf, a cousin of Syrian former president Bashar al-Assad.The company had approximately 3,500 employees and 8 million customers as of 2016.It is headquartered on Sehnaya Road in Damascus\n",
    "\n",
    "**Stakeholders:** SyriaTel's management, marketing, and customer service teams.\n",
    "\n",
    "**Objective:** Build a classification model to predict whether a customer will churn based on provided features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711c9fd",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "We will analyze the dataset, check for missing values, and understand feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27966289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96bbca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account length</th>\n",
       "      <th>area code</th>\n",
       "      <th>phone number</th>\n",
       "      <th>international plan</th>\n",
       "      <th>voice mail plan</th>\n",
       "      <th>number vmail messages</th>\n",
       "      <th>total day minutes</th>\n",
       "      <th>total day calls</th>\n",
       "      <th>total day charge</th>\n",
       "      <th>...</th>\n",
       "      <th>total eve calls</th>\n",
       "      <th>total eve charge</th>\n",
       "      <th>total night minutes</th>\n",
       "      <th>total night calls</th>\n",
       "      <th>total night charge</th>\n",
       "      <th>total intl minutes</th>\n",
       "      <th>total intl calls</th>\n",
       "      <th>total intl charge</th>\n",
       "      <th>customer service calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>382-4657</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>371-7191</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>358-1921</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>375-9999</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>330-6626</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  account length  area code phone number international plan  \\\n",
       "0    KS             128        415     382-4657                 no   \n",
       "1    OH             107        415     371-7191                 no   \n",
       "2    NJ             137        415     358-1921                 no   \n",
       "3    OH              84        408     375-9999                yes   \n",
       "4    OK              75        415     330-6626                yes   \n",
       "\n",
       "  voice mail plan  number vmail messages  total day minutes  total day calls  \\\n",
       "0             yes                     25              265.1              110   \n",
       "1             yes                     26              161.6              123   \n",
       "2              no                      0              243.4              114   \n",
       "3              no                      0              299.4               71   \n",
       "4              no                      0              166.7              113   \n",
       "\n",
       "   total day charge  ...  total eve calls  total eve charge  \\\n",
       "0             45.07  ...               99             16.78   \n",
       "1             27.47  ...              103             16.62   \n",
       "2             41.38  ...              110             10.30   \n",
       "3             50.90  ...               88              5.26   \n",
       "4             28.34  ...              122             12.61   \n",
       "\n",
       "   total night minutes  total night calls  total night charge  \\\n",
       "0                244.7                 91               11.01   \n",
       "1                254.4                103               11.45   \n",
       "2                162.6                104                7.32   \n",
       "3                196.9                 89                8.86   \n",
       "4                186.9                121                8.41   \n",
       "\n",
       "   total intl minutes  total intl calls  total intl charge  \\\n",
       "0                10.0                 3               2.70   \n",
       "1                13.7                 3               3.70   \n",
       "2                12.2                 5               3.29   \n",
       "3                 6.6                 7               1.78   \n",
       "4                10.1                 3               2.73   \n",
       "\n",
       "   customer service calls  churn  \n",
       "0                       1  False  \n",
       "1                       1  False  \n",
       "2                       0  False  \n",
       "3                       2  False  \n",
       "4                       3  False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('SyriaTel_Customer_Churn.csv')\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901aea45",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- Handling missing values (if any)\n",
    "- Encoding categorical variables\n",
    "- Splitting data into train and test sets\n",
    "- Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ddf69aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state                     0\n",
       "account length            0\n",
       "area code                 0\n",
       "phone number              0\n",
       "international plan        0\n",
       "voice mail plan           0\n",
       "number vmail messages     0\n",
       "total day minutes         0\n",
       "total day calls           0\n",
       "total day charge          0\n",
       "total eve minutes         0\n",
       "total eve calls           0\n",
       "total eve charge          0\n",
       "total night minutes       0\n",
       "total night calls         0\n",
       "total night charge        0\n",
       "total intl minutes        0\n",
       "total intl calls          0\n",
       "total intl charge         0\n",
       "customer service calls    0\n",
       "churn                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b858f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "\n",
    "# Remove spaces in column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])  \n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['churn'])  \n",
    "y = df['churn']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a35a1",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Training multiple classification models:\n",
    "- Logistic Regression (Baseline Model)\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81ab730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.98      0.92       566\n",
      "        True       0.57      0.17      0.26       101\n",
      "\n",
      "    accuracy                           0.85       667\n",
      "   macro avg       0.72      0.57      0.59       667\n",
      "weighted avg       0.82      0.85      0.82       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "print('Logistic Regression Evaluation')\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a3b1904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Tuned Logistic Regression Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.98      0.92       566\n",
      "        True       0.57      0.17      0.26       101\n",
      "\n",
      "    accuracy                           0.85       667\n",
      "   macro avg       0.72      0.57      0.59       667\n",
      "weighted avg       0.82      0.85      0.82       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model Tuning\n",
    "#Logistic Regression model using GridSearchCV to optimize hyperparameters:\n",
    "\n",
    "# (i) C (Regularization Strength): Controls the trade-off between bias and variance.\n",
    "#(ii) Penalty (Regularization Type): L1 (Lasso) or L2 (Ridge).\n",
    "#(iii) Solver: Algorithm used for optimization.\n",
    "\n",
    "# Define parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Type of regularization\n",
    "    'solver': ['liblinear']  # Supports L1 and L2 regularization\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search_log = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search_log.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters for Logistic Regression:\", grid_search_log.best_params_)\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_log_model = grid_search_log.best_estimator_\n",
    "y_pred_log_tuned = best_log_model.predict(X_test)\n",
    "\n",
    "print('Tuned Logistic Regression Evaluation')\n",
    "print(classification_report(y_test, y_pred_log_tuned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f362c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.98      0.96       566\n",
      "        True       0.88      0.67      0.76       101\n",
      "\n",
      "    accuracy                           0.94       667\n",
      "   macro avg       0.91      0.83      0.86       667\n",
      "weighted avg       0.93      0.94      0.93       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print('Decision Tree Evaluation')\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08d3f996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.98      0.96       566\n",
      "        True       0.88      0.66      0.76       101\n",
      "\n",
      "    accuracy                           0.94       667\n",
      "   macro avg       0.91      0.82      0.86       667\n",
      "weighted avg       0.93      0.94      0.93       667\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier model Tuning\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search with Cross Validation\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Tuned Decision Tree Evaluation')\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Print best parameters\n",
    "print('Best Parameters:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a717e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Evaluation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97       566\n",
      "        True       0.92      0.69      0.79       101\n",
      "\n",
      "    accuracy                           0.94       667\n",
      "   macro avg       0.93      0.84      0.88       667\n",
      "weighted avg       0.94      0.94      0.94       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print('Random Forest Evaluation')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8694e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier model Tuning\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required in a leaf node\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider at each split\n",
    "    'bootstrap': [True, False]  # Whether to use bootstrapping\n",
    "}\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search with Cross Validation\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Tuned Random Forest Evaluation')\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Print best parameters\n",
    "print('Best Parameters:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Plot\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feature_importance, y=feature_importance.index, palette=\"viridis\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876f4fc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Comparing the models based on accuracy, precision, recall, and F1-score.\n",
    "A confusion matrix will help visualize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70619060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression')\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', ax=axes[1])\n",
    "axes[1].set_title('Decision Tree')\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', ax=axes[2])\n",
    "axes[2].set_title('Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35187ab",
   "metadata": {},
   "source": [
    "## Findings & Recommendations\n",
    "- **Best model:** Based on evaluation metrics\n",
    "    \n",
    "    1.  Logistic Regression had high accuracy but struggled with recall for churned customers (class 1).\n",
    "    2. Decision Tree Classifier performed better in recall but still had room for improvement.\n",
    "    3. Random Forest Classifier had the highest overall precision and recall, making it the most effective model for churn prediction.\n",
    "    - Best Model: Random Forest Classifier\n",
    "    \n",
    "\n",
    "- **Business Insights:** Factors influencing churn are:\n",
    "\n",
    "    1. Customer Service Calls â€“ Customers with more service calls were more likely to churn.\n",
    "    2. International Plan â€“ Customers with an international plan showed a different churn pattern, requiring further analysis.\n",
    "    3. Total Minutes & Charges â€“ Higher usage in day/night calls correlated with churn, possibly due to high costs.\n",
    "    4. Voice Mail Plan â€“ Customers without a voicemail plan showed different churn behavior.\n",
    "    \n",
    "    \n",
    "- **Recommendations for Retention Strategies:** Below are the recommedations based on the models:\n",
    "     1. Improve Customer Support â€“ Since frequent customer service calls are linked to churn, enhancing support quality and reducing call times can improve retention.\n",
    "     2. Personalized Plans â€“ Offering discounts or flexible pricing for heavy users could help retain high-usage customers.\n",
    "     3. Targeted Promotions â€“ Providing exclusive benefits to international plan users may increase satisfaction and reduce churn.\n",
    "     4. Churn Prevention Alerts â€“ Implementing AI-driven alerts to detect early churn signals and intervene proactively.\n",
    "\n",
    "\n",
    "- **Next Steps:** \n",
    "\n",
    "    1. Hyperparameter Tuning â€“ Optimize Random Forest with GridSearchCV to improve performance.\n",
    "    2. Feature Engineering â€“ Create new features, such as customer tenure or engagement metrics.\n",
    "    3. Testing More Models â€“ Try Gradient Boosting, XGBoost, or Neural Networks for further improvements.\n",
    "    4. Deploy Model in Production â€“ Implement an API or integrate with SyriaTelâ€™s CRM for real-time churn prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440e9dd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, we successfully built a customer churn prediction model for SyriaTel using machine learning. Our analysis identified key factors influencing churn, allowing us to develop actionable strategies for customer retention.\n",
    "\n",
    "After evaluating multiple models, the Random Forest Classifier emerged as the best performer, offering a strong balance of precision, recall, and overall accuracy. Key insights from the data revealed that customer service interactions, international plans, and high call charges significantly impact churn behavior.\n",
    "\n",
    "To improve customer retention, we recommend enhancing customer support, offering personalized pricing plans, and deploying AI-driven churn prevention alerts. Further improvements can be made through hyperparameter tuning, additional feature engineering, and testing advanced models like XGBoost.\n",
    "\n",
    "Moving forward, deploying the model into SyriaTelâ€™s CRM system for real-time churn prediction will help the company take proactive steps to reduce customer attrition and boost customer loyalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614592db",
   "metadata": {},
   "source": [
    " ## Supporting Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Data (Assuming df is already loaded)\n",
    "df.columns = df.columns.str.strip()  # Remove spaces from column names\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, drop_first=True)  # One-hot encoding\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['churn'])\n",
    "y = df['churn']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# ---------------------- Feature Importance Plot ----------------------\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feature_importance, y=feature_importance.index, palette=\"viridis\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------- Confusion Matrix ----------------------\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------- Churn Distribution ----------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df['churn'], palette=\"coolwarm\")\n",
    "plt.title(\"Churn Distribution\")\n",
    "plt.xlabel(\"Churn (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------- Total Day Minutes vs Churn ----------------------\n",
    "if 'total day minutes' in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(x=df['churn'], y=df['total day minutes'], palette=\"coolwarm\")\n",
    "    plt.title(\"Total Day Minutes vs Churn\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Column 'total day minutes' not found in dataset.\")\n",
    "\n",
    "# ---------------------- ROC Curve for Model Comparison ----------------------\n",
    "# Train other models\n",
    "log_reg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob_rf = rf.predict_proba(X_test)[:,1]\n",
    "y_pred_prob_log = log_reg.predict_proba(X_test)[:,1]\n",
    "y_pred_prob_dt = dt.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_prob_rf)\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_pred_prob_log)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_prob_dt)\n",
    "\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "auc_log = auc(fpr_log, tpr_log)\n",
    "auc_dt = auc(fpr_dt, tpr_dt)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {auc_rf:.2f})\", color='blue')\n",
    "plt.plot(fpr_log, tpr_log, label=f\"Logistic Regression (AUC = {auc_log:.2f})\", color='green')\n",
    "plt.plot(fpr_dt, tpr_dt, label=f\"Decision Tree (AUC = {auc_dt:.2f})\", color='red')\n",
    "plt.plot([0,1], [0,1], 'k--')  # Random model line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7d67b",
   "metadata": {},
   "source": [
    "## Other Important Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Churn Distribution (Bar Chart)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y, palette=\"coolwarm\")\n",
    "plt.title(\"Churn Distribution\")\n",
    "plt.xlabel(\"Churn (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb22b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Correlation Heatmap\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a35b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tenure (Account Length) vs. Churn (Box Plot)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=y, y=df[\"account length\"], palette=\"coolwarm\")\n",
    "plt.title(\"Account Length vs. Churn\")\n",
    "plt.xlabel(\"Churn (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Account Length\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Monthly Charges Distribution by Churn (Histogram)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df, x=\"total day charge\", hue=\"churn\", bins=30, kde=True, palette=\"coolwarm\", alpha=0.6)\n",
    "plt.title(\"Monthly Charges Distribution by Churn\")\n",
    "plt.xlabel(\"Total Day Charge\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
